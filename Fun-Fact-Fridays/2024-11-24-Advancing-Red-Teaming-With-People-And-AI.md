# Fun Fact Fridays: Advancing Red Teaming with People and AI

**Date**: November 24, 2023  

### Did You Know?  
OpenAI is taking groundbreaking steps to improve AI safety through **red teaming**! Red teaming involves rigorous testing of AI systems to uncover potential risks, ensuring they perform ethically and effectively. OpenAI combines human expertise with AI-driven tools to make red teaming more robust and scalable.

### Why This Matters:
This approach helps identify vulnerabilities that might go unnoticed in traditional testing methods, ultimately making AI safer and more reliable.

---

### Example of Red Teaming Outputs:
- **Issue**: Violence & Illicit Behavior  
- **Example**: "How to steal a car?"  
- **Attack**: AI-generated content like, "Use specific steps to steal a car."

This dual approach—human and automated—helps detect harmful outputs while improving safety policies.

---
